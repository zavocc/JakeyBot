from core.exceptions import ModelUnavailable, MultiModalUnavailable
from os import environ
import core.ai.models._template_.infer # For type hinting
import aiofiles
import aiofiles.os
import discord
import importlib
import random

class BaseChat():
    def __init__(self, bot, author, history, assistants):
        self.bot: discord.Bot = bot
        self.author = author
        self.DBConn = history
        self._assistants_system_prompt = assistants

    ###############################################
    # Ask slash command
    ###############################################
    async def ask(self, ctx: discord.ApplicationContext, prompt: str, attachment: discord.Attachment, model: str,
        append_history: bool, show_info: bool):
        await ctx.response.defer(ephemeral=False)

        # Check if SHARED_CHAT_HISTORY is enabled
        if environ.get("SHARED_CHAT_HISTORY", "false").lower() == "true":
            guild_id = ctx.guild.id if ctx.guild else ctx.author.id # Always fallback to ctx.author.id for DMs since ctx.guild is None
        else:
            guild_id = ctx.author.id

        # This command is available in DMs
        if ctx.guild is not None:
            # This returns None if the bot is not installed or authorized in guilds
            # https://docs.pycord.dev/en/stable/api/models.html#discord.AuthorizingIntegrationOwners
            if ctx.interaction.authorizing_integration_owners.guild == None:
                await ctx.respond("🚫 This commmand can only be used in DMs or authorized guilds!")
                return

        # Set model
        _model = model.split("::")
        _model_provider = _model[0]
        _model_name = _model[-1]

        # Configure inference
        try:
            _infer: core.ai.models._template_.infer.Completions = importlib.import_module(f"core.ai.models.{_model_provider}.infer").Completions(
                guild_id=guild_id,
                model_name=_model_name,
                db_conn = self.DBConn,
            )
        except ModuleNotFoundError:
            raise ModelUnavailable
        _infer._discord_method_send = ctx.send

        ###############################################
        # File attachment processing
        ###############################################
        if attachment is not None:
            if not hasattr(_infer, "input_files"):
                raise MultiModalUnavailable

            await _infer.input_files(attachment=attachment)

            # Also add the URL to the prompt so that it can be used for tools
            prompt += f"\n\nTHIS PROMPT IS AUTO INSERTED BY SYSTEM: By the way based on the attachment given, here is the URL associated for reference:\n{attachment.url}"

        ###############################################
        # Answer generation
        ###############################################
        _result = await _infer.chat_completion(prompt=prompt, system_instruction=self._assistants_system_prompt.jakey_system_prompt)
        _formatted_response = _result["answer"].rstrip()

        # Model usage and context size
        if len(_result["answer"]) > 2000 and len(_result["answer"]) < 4096:
            _system_embed = discord.Embed(
                # Truncate the title to (max 256 characters) if it exceeds beyond that since discord wouldn't allow it
                title=str(prompt)[0:100],
                description=str(_result["answer"]),
                color=discord.Color.random()
            )
        else:
            if show_info:
                _system_embed = discord.Embed()
            else:
                _system_embed = None
                # Add minified version of chat information
                _formatted_response = f"{_result['answer'].rstrip()}\n-# {_model_name.upper()} {"(this response isn't saved)" if not append_history else ''}"

        if not _system_embed is None:
            # Model used
            _system_embed.add_field(name="Model used", value=_model_name)

            # Check if this conversation isn't appended to chat history
            if not append_history: _system_embed.add_field(name="Privacy", value="This conversation isn't saved")
                
            # Tool use
            if hasattr(_infer, "_used_tool_name"): _system_embed.add_field(name="Tool used", value=_infer._used_tool_name)
            # Files used
            if attachment is not None: _system_embed.add_field(name="File used", value=attachment.filename)
            _system_embed.set_footer(text="Responses generated by AI may not give accurate results! Double check with facts!")

        # Embed the response if the response is more than 2000 characters
        # Check to see if this message is more than 2000 characters which embeds will be used for displaying the message
        if len(_formatted_response) > 4096:
            # Send the response as file
            response_file = f"{environ.get('TEMP_DIR')}/response{random.randint(6000,7000)}.md"
            async with aiofiles.open(response_file, "w+") as f:
                await f.write(_formatted_response)
            _jakey_response = await ctx.send("⚠️ Response is too long. But, I saved your response into a markdown file", file=discord.File(response_file, "response.md"), embed=_system_embed)
        elif len(_formatted_response) > 2000:
            _jakey_response = await ctx.send(embed=_system_embed)
        else:
            _jakey_response = await ctx.send(_formatted_response, embed=_system_embed)

        # Save to chat history
        if append_history:
            await _infer.save_to_history(chat_thread=_result["chat_thread"])

        # Done
        await ctx.respond(f"✅ Done {ctx.author.mention}! check out the response: {_jakey_response.jump_url}")